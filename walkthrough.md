## NomicRAG

### Project Introduction

NomicRAG is an open-source Retrieval-Augmented Generation (RAG) application designed to leverage Nomic's new
long-context embedding model, ChromaDB, and Ollama. This project demonstrates the integration of advanced tools to
create a powerful search-enabled chatbot capable of processing and retrieving relevant documents efficiently. With an
expanded context window of up to 8,000 tokens, NomicRAG can handle large document chunks, providing detailed and
contextually rich responses. The app is deployed using LangServe, offering an interactive UI for seamless user
interaction and testing. NomicRAG represents a cutting-edge solution for modern web search and document retrieval needs.

### Prerequisites

- Python 3.11
- pip (Python package installer)
- Git (optional)

### Step 1: Initial Setup

#### 1. Initialize the Environment

First, let's set up the environment and install necessary dependencies.

1. **Create a `.env` file:**

2. This file will store your API keys and other configuration settings. Ensure it is included in your `.gitignore` file
   to prevent it from being committed to your repository.

   Example `.env` file:
   ```plaintext
   LANGCHAIN_API_KEY="your_langchain_api_key"
   LANGCHAIN_TRACING_V2=true
   LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
   LANGCHAIN_PROJECT="NomicRAG"
   
   OPENAI_API_KEY="your_open_api_key"
   ```

2. **Install required packages:**
   ```bash
   pip install langchain langchain_community openai streamlit python-dotenv
   ```
   ```bash
   pip install -U langchain-cli
   ```
    ```bash
   pip install langchain-nomic
   ```
   ```bash
   pip install tiktoken
   ```
   ```bash
   pip install chromadb
   ```

#### Key Concepts

##### 1. LangChain-Nomic Integration

- **Definition**: `langchain-nomic` is a package that integrates Nomic's advanced data visualization and analysis tools
  with LangChain applications. Nomic provides tools for exploring, understanding, and visualizing large language model
  outputs and embeddings.
- **Usage**: By using the `langchain-nomic` package, developers can leverage Nomic's visualization capabilities to
  better understand and present the outputs from their LangChain applications. This integration enhances the ability to
  analyze and interpret the results generated by large language models.

##### 2. Tiktoken

- **Definition**: `tiktoken` is a fast byte pair encoding (BPE) tokenizer used for tokenizing text for language models,
  especially those from OpenAI. It helps in converting text into tokens that can be processed by models.
- **Usage**: `tiktoken` is used in LangChain applications to efficiently tokenize text inputs and outputs. This ensures
  that the text is properly formatted for processing by language models, optimizing performance and accuracy.

##### 3. ChromaDB

- **Definition**: `chromadb` is a database designed for storing and querying vector embeddings. It is optimized for
  performance and scalability, making it suitable for applications involving large-scale embedding-based retrieval and
  analysis.
- **Usage**: In LangChain applications, `chromadb` is used to store and manage vector embeddings generated by language
  models. This allows for efficient retrieval and analysis of embeddings, enabling advanced search and retrieval
  capabilities based on vector similarity.

### Step 2: Setup LangServe and LangSmith

#### 1. LangServe Setup

Set up LangServe to manage our application deployment.
Use the LangServe CLI to create a new application called `pinecone-serverless`.

```bash
langchain app new nomic-rag
```   

#### 2. LangSmith Setup

Make sure u have created a LangSmith project for this lab.

**Project Name:** NomicRAG

### Step 3: Setup Nomic

#### 1: Create a Account

- **Access Exa:**

  Navigate to [Nomic](https://atlas.nomic.ai/cli-login).

  <img src="https://i.imghippo.com/files/eTraW1718273708.jpg" alt="" border="0">

#### 2: Get Your Own API Key

1. **Navigate to API Keys:**

   <img src="https://i.imghippo.com/files/jLxgE1718273856.jpg" alt="" border="0">
   <img src="https://i.imghippo.com/files/gsXJj1718273891.jpg" alt="" border="0">
   <img src="https://i.imghippo.com/files/2eVwV1718273981.jpg" alt="" border="0">

2 **Login to nomic by your API key:**

   ```bash
   nomic login your_nomic_api_key
   ```

#### Key Concepts

##### 1. Nomic Account Setup

- **Definition**: Setting up a Nomic account is the first step to utilizing Nomic's capabilities for your projects. It
  involves creating an account, obtaining an API key, and configuring your environment to interact with Nomic's
  services.
- **Usage**: An account is necessary to manage and access Nomic's features, including data visualization, analysis, and
  API services. This setup allows you to authenticate and interact with Nomic's platform securely.

##### 2. Nomic API Key

- **Definition**: An API key is a unique identifier used to authenticate requests associated with your Nomic account. It
  is essential for interacting with Nomic's services securely.
- **Usage**: The API key is used during the login process to enable your application to communicate with Nomic's API. It
  provides the necessary permissions to access and manage your projects.

### Step 4: Add Web Content Retrieval and Vector Store Integration with Chroma and NomicEmbeddings

In this step, we will integrate web content retrieval and vector store functionality using Chroma and NomicEmbeddings.
This will enable our application to retrieve, split, and store web content for efficient search and retrieval.

#### 1. Create `chain.py` to Integrate Web Content Retrieval and Vector Store

Here, we will set up the necessary components to retrieve web content, split it into manageable chunks, and store the
embeddings in a vector store for retrieval.

**File**: `nomic-rag/app/chain.py`

**Code for `chain.py`:**

```python
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_nomic import NomicEmbeddings
from langchain_nomic.embeddings import NomicEmbeddings

from dotenv import load_dotenv
from langchain.callbacks.tracers.langchain import wait_for_all_tracers

load_dotenv()
wait_for_all_tracers()

# Web content retrieval, aggregation, and character-based document splitting.
urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]

docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size = 7500, chunk_overlap = 100
)
doc_splits = text_splitter.split_documents(docs_list)

# Add to vectorDB
vectorstore = Chroma.from_documents(
    documents = doc_splits,
    collection_name = "rag-chroma",
    embedding = NomicEmbeddings(model = "nomic-embed-text-v1"),
)
retriever = vectorstore.as_retriever()

print(retriever.get_relevant_documents("Talk decomposition"))
```

In this `chain.py` file:

- **WebBaseLoader** is used to retrieve content from specified URLs.
- **CharacterTextSplitter** is utilized to split the retrieved documents into smaller chunks for efficient processing.
- **Chroma** is employed as the vector store to store and manage the document embeddings.
- **NomicEmbeddings** provides the embeddings for the documents, which are stored in the vector store.
- Finally, a retriever is created to fetch relevant documents based on queries.

#### 2. Test the Chain

Run the `chain.py` file and inspect the results:

<img src="https://i.imghippo.com/files/470oL1718280922.jpg" alt="" border="0">

This output indicates that the chain is working correctly, retrieving and processing the web content, and storing the
embeddings in the vector store for efficient retrieval.

### Step 5: Add Web Content Retrieval and Vector Store Integration with Chroma and NomicEmbeddings

In this step, we will integrate web content retrieval and vector store functionality using Chroma and NomicEmbeddings.
This will enable our application to retrieve, split, and store web content for efficient search and retrieval.

#### 1. Set Up Web Content Retrieval and Vector Store

1. **Install Ollama:**

First, install Ollama from the official repository:

```bash
ollama pull mistral:instruct
```

You can find the installation instructions on
the [Ollama GitHub page](https://github.com/ollama/ollama?tab=readme-ov-file).

<img src="https://i.imghippo.com/files/uKGgC1718281953.jpg" alt="" border="0">

2. **Pull the Model:**

After installing Ollama, pull the model:

```bash
ollama pull mistral:instruct
```

#### 2. Integrate Web Content Retrieval and Vector Store

Create a `chain.py` file to integrate web content retrieval and vector store functionality.

**File**: `nomic-rag/app/chain.py`

**Code for `chain.py`:**

```python
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_nomic import NomicEmbeddings
from langchain_nomic.embeddings import NomicEmbeddings
from langchain_community.chat_models import ChatOllama

from dotenv import load_dotenv
from langchain.callbacks.tracers.langchain import wait_for_all_tracers

load_dotenv()
wait_for_all_tracers()

# Web content retrieval, aggregation, and character-based document splitting.
urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]

docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size = 7500, chunk_overlap = 100
)
doc_splits = text_splitter.split_documents(docs_list)

# Add to vectorDB
vectorstore = Chroma.from_documents(
    documents = doc_splits,
    collection_name = "rag-chroma",
    embedding = NomicEmbeddings(model = "nomic-embed-text-v1"),
)
retriever = vectorstore.as_retriever()

# Chain block
template = """
Answer the question based only on the following context:
{context}
Question: 
{question}
"""
prompt = ChatPromptTemplate.from_template(template)

chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        } | prompt | ChatOllama(model = "mistral:instruct") | StrOutputParser()
)

if __name__ == "__main__":
    print(chain.invoke("What are types of agent memory?"))
```

#### 3. Test the Chain

Run the `chain.py` file and inspect the results:

<img src="https://i.imghippo.com/files/7fnKv1718283816.jpg" alt="" border="0">

#### The Whole Workflow

The whole workflow involves:

1. Retrieving and splitting documents.
2. Embedding and indexing them using Nomic embeddings.
3. Running an LLM to process them and provide answers.

#### Key Concepts

##### 1. ChatOllama

- **Definition**: `ChatOllama` is a language model used for generating responses based on a given context. It integrates
  with LangChain applications to provide advanced natural language processing capabilities.
- **Usage**: In this application, `ChatOllama` is used to generate answers based on the context retrieved from the
  vector store. This enhances the chatbot's ability to provide accurate and contextually relevant responses.

### Step 6: Serve the Application Using LangServe

#### 1. Update `server.py`:

```python
...
from app.chain import chain

...
add_routes(app, chain, path = "/nomic-rag")
...
```

#### 2. Update `chain.py`:

```python
chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        } | prompt | ChatOllama(model = "mistral:instruct") | StrOutputParser()
).with_types(input_type = str)
```

#### 3. Serving the Application by LangServe

Run the following commands to set up and serve the application using LangServe.

   ```bash
   cd nomic-rag
   langchain serve
   ```

You can now access the application through the following links:

Access [Playground](http://127.0.0.1:8000/nomic-rag/playground/)

<img src="https://i.imghippo.com/files/jbfdW1718284923.jpg" alt="" border="0">